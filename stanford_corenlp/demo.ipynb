{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo follows https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "\n",
    "You can compare it against https://corenlp.run/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "port = 10002\n",
    "url = f'http://localhost:{port}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['The']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('VP', [Tree('VBZ', ['jumps']), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])]), Tree('.', ['.'])])])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"The quick brown fox jumps over the lazy dog.\"\n",
    "list(parser.parse(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(parser.raw_parse('What is the airspeed of an unladen swallow ?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           ROOT                            \n",
      "                            |                               \n",
      "                          SBARQ                            \n",
      "  __________________________|____________________________   \n",
      " |        SQ                                             | \n",
      " |     ___|_________________                             |  \n",
      " |    |                     NP                           | \n",
      " |    |        _____________|______________________      |  \n",
      " |    |       |                  PP                S     | \n",
      " |    |       |              ____|___              |     |  \n",
      "WHNP  |       NP            |        NP            VP    | \n",
      " |    |    ___|_____        |     ___|_____        |     |  \n",
      " WP  VBZ  DT        NN      IN   DT        JJ      VB    . \n",
      " |    |   |         |       |    |         |       |     |  \n",
      "What  is the     airspeed   of   an     unladen swallow  ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = next(parser.raw_parse('What is the airspeed of an unladen swallow ?'))\n",
    "res.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "dep_parser = CoreNLPDependencyParser(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "In sentence 0\n",
      "('jumps', 'VBZ') nsubj ('fox', 'NN')\n",
      "('fox', 'NN') det ('The', 'DT')\n",
      "('fox', 'NN') amod ('quick', 'JJ')\n",
      "('fox', 'NN') amod ('brown', 'JJ')\n",
      "('jumps', 'VBZ') nmod ('dog', 'NN')\n",
      "('dog', 'NN') case ('over', 'IN')\n",
      "('dog', 'NN') det ('the', 'DT')\n",
      "('dog', 'NN') amod ('lazy', 'JJ')\n",
      "('jumps', 'VBZ') punct ('.', '.')\n"
     ]
    }
   ],
   "source": [
    "print(sent)\n",
    "result = dep_parser.parse(sent.split())\n",
    "for i, a in enumerate(result):\n",
    "    print(f'In sentence {i}')\n",
    "    for governor, dep, dependent in a.triples():\n",
    "        print(governor, dep, dependent) \n",
    "# [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in the text:\n",
      "Apple (ORGANIZATION)\n",
      "U.K. (LOCATION)\n",
      "$ (MONEY)\n",
      "1 (MONEY)\n",
      "billion (MONEY)\n",
      "Tim (PERSON)\n",
      "Cook (PERSON)\n",
      "CEO (TITLE)\n",
      "London (CITY)\n",
      "next (DATE)\n",
      "week (DATE)\n",
      "\n",
      "Structured Entities:\n",
      "ORGANIZATION: Apple\n",
      "LOCATION: U.K.\n",
      "MONEY: $, 1, billion\n",
      "PERSON: Tim, Cook\n",
      "TITLE: CEO\n",
      "CITY: London\n",
      "DATE: next, week\n"
     ]
    }
   ],
   "source": [
    "# Ensure the Stanford CoreNLP server is running on localhost:9000\n",
    "ner_parser = CoreNLPParser(url=url, tagtype='ner')\n",
    "\n",
    "# Input text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion. Tim Cook will be meeting with the company's CEO in London next week.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Perform NER\n",
    "ner_tagged = ner_parser.tag(tokens)\n",
    "\n",
    "# Print the recognized entities\n",
    "print(\"Entities in the text:\")\n",
    "for word, tag in ner_tagged:\n",
    "    if tag != 'O':  # 'O' means no entity\n",
    "        print(f\"{word} ({tag})\")\n",
    "\n",
    "# Example to show how to extract entities in a more structured way\n",
    "entities = {}\n",
    "for word, tag in ner_tagged:\n",
    "    if tag != 'O':  # 'O' means no entity\n",
    "        if tag not in entities:\n",
    "            entities[tag] = []\n",
    "        entities[tag].append(word)\n",
    "\n",
    "# Print structured entities\n",
    "print(\"\\nStructured Entities:\")\n",
    "for entity_type, words in entities.items():\n",
    "    print(f\"{entity_type}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
